version: '3.8'

services:
  # Main Application - AI Software Development Crew Web UI
  ui:
    build:
      context: .
      dockerfile: Containerfile
    container_name: ui
    ports:
      - "8080:8080"
    environment:
      # LLM Configuration
      - LLM_ENVIRONMENT=${LLM_ENVIRONMENT:-production}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.containers.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:latest}
      
      # Model Selection (for OpenRouter or native providers)
      - LLM_MODEL_MANAGER=${LLM_MODEL_MANAGER:-x-ai/grok-4.1-fast:free}
      - LLM_MODEL_WORKER=${LLM_MODEL_WORKER:-x-ai/grok-4.1-fast:free}
      - LLM_MODEL_REVIEWER=${LLM_MODEL_REVIEWER:-x-ai/grok-4.1-fast:free}
      
      # Native Provider API Keys (if not using OpenRouter)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      
      # Project Configuration
      - PROJECT_ID=${PROJECT_ID:-default-project}
      - WORKSPACE_PATH=/app/workspace
      - PYTHONPATH=/app/src
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      
      # Retry Configuration
      - MAX_RETRY_ATTEMPTS=${MAX_RETRY_ATTEMPTS:-100}
      
      # Git Configuration (optional)
      - ENABLE_GIT=${ENABLE_GIT:-true}
    volumes:
      # Persist workspace data
      - ./workspace:/app/workspace
      # Optional: Mount .env file if you prefer (comment out if using environment section above)
      # - ./.env:/app/.env:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/stats"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - ai-dev-network
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"  # Not needed for Podman 4+

  # Crew Service - CLI/Worker container
  crew:
    build:
      context: .
      dockerfile: Containerfile
    container_name: crew
    command: tail -f /dev/null  # Keep container running for CLI access
    environment:
      # LLM Configuration
      - LLM_ENVIRONMENT=${LLM_ENVIRONMENT:-production}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.containers.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:latest}
      
      # Model Selection
      - LLM_MODEL_MANAGER=${LLM_MODEL_MANAGER:-x-ai/grok-4.1-fast:free}
      - LLM_MODEL_WORKER=${LLM_MODEL_WORKER:-x-ai/grok-4.1-fast:free}
      - LLM_MODEL_REVIEWER=${LLM_MODEL_REVIEWER:-x-ai/grok-4.1-fast:free}
      
      # APIs
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      
      # Config
      - PROJECT_ID=${PROJECT_ID:-default-project}
      - WORKSPACE_PATH=/app/workspace
      - PYTHONPATH=/app/src
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_RETRY_ATTEMPTS=${MAX_RETRY_ATTEMPTS:-100}
      - ENABLE_GIT=${ENABLE_GIT:-true}
    volumes:
      - ./workspace:/app/workspace
    restart: unless-stopped
    networks:
      - ai-dev-network
    # extra_hosts:
    #   - "host.docker.internal:host-gateway"

  # Documentation - Docusaurus Documentation Site
  docs:
    build:
      context: ./docs
      dockerfile: Dockerfile
    container_name: ai-dev-docs
    ports:
      - "3000:3000"
    volumes:
      # Mount docs directory for live reloading during development
      - ./docs:/app
      # Exclude node_modules from host mount to use container's installed modules
      - /app/node_modules
    environment:
      - NODE_ENV=development
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - ai-dev-network

  # Optional: Dragonfly - Redis-compatible cache (for future distributed setup)
  # Uncomment if you need caching
  # cache:
  #   image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
  #   container_name: ai-dev-cache
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - dragonfly-data:/data
  #   command: >
  #     --requirepass ${REDIS_PASSWORD:-}
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 5s
  #     timeout: 3s
  #     retries: 5
  #   networks:
  #     - ai-dev-network

  # Optional: RabbitMQ - Message queue (for future distributed setup)
  # Uncomment if you need message queuing
  # rabbitmq:
  #   image: rabbitmq:3-management
  #   container_name: ai-dev-rabbitmq
  #   ports:
  #     - "5672:5672"
  #     - "15672:15672"  # Management UI
  #   environment:
  #     RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-guest}
  #     RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-guest}
  #   volumes:
  #     - rabbitmq-data:/var/lib/rabbitmq
  #   healthcheck:
  #     test: ["CMD", "rabbitmq-diagnostics", "ping"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #   networks:
  #     - ai-dev-network

  # Optional: PostgreSQL - Persistent storage (for future distributed setup)
  # Uncomment if you need database
  # postgres:
  #   image: postgres:16-alpine
  #   container_name: ai-dev-postgres
  #   ports:
  #     - "5432:5432"
  #   environment:
  #     POSTGRES_DB: ${POSTGRES_DB:-ai_dev_crew}
  #     POSTGRES_USER: ${POSTGRES_USER:-postgres}
  #     POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
  #   volumes:
  #     - postgres-data:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
  #     interval: 5s
  #     timeout: 3s
  #     retries: 5
  #   networks:
  #     - ai-dev-network

# volumes:
  # Uncomment if using infrastructure services
  # dragonfly-data:
  # rabbitmq-data:
  # postgres-data:

networks:
  ai-dev-network:
    driver: bridge

