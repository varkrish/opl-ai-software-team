# ─────────────────────────────────────────────────────────────────────────────
# AI Crew Studio — Environment Variables
# Copy this file to .env and fill in your values
# ─────────────────────────────────────────────────────────────────────────────

# ── LLM Configuration ──────────────────────────────────────────────────────
# The backend reads LLM settings from config.yaml (mounted into the container).
# See agent/config.example.yaml for the full config file format.
#
# For quick local dev you can also export these env vars instead:
#   export LLM_API_KEY=your_key
#   export LLM_API_BASE_URL=https://litellm-prod.apps.maas.redhatworkshops.io

# ── Config file path (mounted into backend container) ────────────────────────
CONFIG_FILE=./config.yaml

# ── Ollama (for local models) ────────────────────────────────────────────────
# Uncomment the ollama service in compose.yaml to use local models
# OLLAMA_BASE_URL=http://ollama:11434

# ── Ports ────────────────────────────────────────────────────────────────────
FRONTEND_PORT=3000
BACKEND_PORT=8080

# ── Flask environment ────────────────────────────────────────────────────────
FLASK_ENV=production
